{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00b73d2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# llm.py\n",
    "\"\"\"\n",
    "Robust LLM wrapper for your pipeline.\n",
    "\n",
    "Features:\n",
    "- DEBUG_STUB mode (controlled via env DEBUG_STUB)\n",
    "- Caching   \n",
    "- Safe JSON extraction when expect_json=True\n",
    "- Graceful handling of missing config / SDK differences\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import hashlib\n",
    "from typing import Optional, Any, List\n",
    "\n",
    "# Try to import OpenAI client; give friendly error if missing\n",
    "try:\n",
    "    from openai import OpenAI\n",
    "except Exception as e:\n",
    "    OpenAI = None\n",
    "    _openai_import_error = e\n",
    "\n",
    "from config import LLM_API_KEY as _CFG_KEY, MODEL as _CFG_MODEL, OPENAI_BASE_URL as _CFG_BASE\n",
    "\n",
    "LLM_API_KEY = _CFG_KEY\n",
    "MODEL = _CFG_MODEL\n",
    "OPENAI_BASE = _CFG_BASE\n",
    "\n",
    "# DEBUG stub mode\n",
    "DEBUG_STUB = False\n",
    "\n",
    "# If not in debug and OpenAI client missing, warn early\n",
    "if not DEBUG_STUB and OpenAI is None:\n",
    "    raise ImportError(\n",
    "        f\"OpenAI client import failed: {_openai_import_error}\\n\"\n",
    "        \"Install the official `openai` Python package (pip install openai) or set DEBUG_STUB=true for local testing.\"\n",
    "    )\n",
    "\n",
    "# Create client lazily (so DEBUG_STUB can run without keys)\n",
    "_client: Optional[Any] = None\n",
    "def _init_client():\n",
    "    global _client\n",
    "    if _client is not None:\n",
    "        return _client\n",
    "    if DEBUG_STUB:\n",
    "        return None\n",
    "    # require API key\n",
    "    if not LLM_API_KEY:\n",
    "        raise RuntimeError(\"LLM_API_KEY not set (env or config).\")\n",
    "    # instantiate OpenAI/OpenRouter client\n",
    "    try:\n",
    "        _client = OpenAI(api_key=LLM_API_KEY, base_url=OPENAI_BASE)\n",
    "    except TypeError:\n",
    "        # Some OpenAI client versions have different param names\n",
    "        _client = OpenAI(api_key=LLM_API_KEY)\n",
    "    return _client\n",
    "\n",
    "# Simple in-memory cache\n",
    "_llm_cache = {}\n",
    "\n",
    "def _cache_key(prompt: str, system: Optional[str]):\n",
    "    key = (system or \"\") + \"\\n\" + prompt\n",
    "    return hashlib.sha256(key.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "def _stub_passage(topic_snippet: str):\n",
    "    text = f\"{topic_snippet} is an important contemporary topic. \" * 30\n",
    "    summary = \"Summary: This passage briefly discusses the topic.\"\n",
    "    return text + \"\\n\\n\" + summary\n",
    "\n",
    "def _extract_json_from_text(text: str):\n",
    "    \"\"\"\n",
    "    Try to find and parse the first JSON object/array in `text`.\n",
    "    Returns parsed object or raises ValueError.\n",
    "    \"\"\"\n",
    "    # look for top-level JSON array or object\n",
    "    m = re.search(r'(\\{.*\\}|\\[.*\\])', text, flags=re.S)\n",
    "    if not m:\n",
    "        raise ValueError(\"No JSON object/array found in LLM output.\")\n",
    "    candidate = m.group(1)\n",
    "    return json.loads(candidate)\n",
    "\n",
    "def _extract_text_from_completion(completion):\n",
    "    \"\"\"\n",
    "    Robust extraction of assistant text from various SDK return shapes.\n",
    "    Supports:\n",
    "      - completion.choices[0].message.content\n",
    "      - completion.choices[0].message (dict)\n",
    "      - completion.choices[0].text\n",
    "      - string (already)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # OpenAI-style: completion.choices[0].message.content\n",
    "        choice = completion.choices[0]\n",
    "        if hasattr(choice, \"message\"):\n",
    "            msg = choice.message\n",
    "            # if msg has .content attribute\n",
    "            content = getattr(msg, \"content\", None)\n",
    "            if content is not None:\n",
    "                return content\n",
    "            # if msg is dict-like\n",
    "            if isinstance(msg, dict) and \"content\" in msg:\n",
    "                return msg[\"content\"]\n",
    "        # fallback to .text (older SDKs)\n",
    "        text = getattr(choice, \"text\", None)\n",
    "        if text:\n",
    "            return text\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # If completion itself is a string\n",
    "    if isinstance(completion, str):\n",
    "        return completion\n",
    "\n",
    "    # Last resort: try to stringify\n",
    "    try:\n",
    "        return json.dumps(completion)\n",
    "    except Exception:\n",
    "        return str(completion)\n",
    "\n",
    "def call_llm(prompt: str, system: Optional[str] = None,\n",
    "             expect_json: bool = False, use_cache: bool = True,\n",
    "             tools: Optional[List[dict]] = None, max_tokens: int = 10000) -> Any:\n",
    "    \"\"\"\n",
    "    Provider-aware LLM call interface.\n",
    "    Supports both OpenAI-style and Bedrock-style models.\n",
    "    \"\"\"\n",
    "    model = MODEL\n",
    "    key = _cache_key(prompt, system)\n",
    "    if use_cache and key in _llm_cache:\n",
    "        return _llm_cache[key]\n",
    "\n",
    "    # DEBUG STUB (skip API call)\n",
    "    if DEBUG_STUB:\n",
    "        low = prompt.lower()\n",
    "        if \"ielts\" in low and (\"passage\" in low or \"academic reading\" in low):\n",
    "            out = _stub_passage(prompt[:80])\n",
    "        elif \"question\" in low or expect_json:\n",
    "            out = json.dumps([\n",
    "                {\"id\": \"MCQ_1\", \"question_text\": \"Stub: What is X?\", \"options\": [\"A\",\"B\",\"C\",\"D\"], \"answer\": \"A\", \"rationale\": \"sample\", \"linked_skills\": [\"Inference\"]},\n",
    "                {\"id\": \"MCQ_2\", \"question_text\": \"Stub: Which is true?\", \"options\": [\"A\",\"B\",\"C\",\"D\"], \"answer\": \"B\", \"rationale\": \"sample\", \"linked_skills\": [\"Scanning\"]}\n",
    "            ])\n",
    "        else:\n",
    "            out = \"FAKE OUTPUT: \" + (prompt[:200] if len(prompt) > 200 else prompt)\n",
    "\n",
    "        if use_cache:\n",
    "            _llm_cache[key] = out\n",
    "        if expect_json:\n",
    "            try:\n",
    "                return json.loads(out)\n",
    "            except Exception:\n",
    "                return out\n",
    "        return out   # ✅ MUST return here\n",
    "\n",
    "\n",
    "    client = _init_client()\n",
    "\n",
    "    messages = []\n",
    "    if system:\n",
    "        messages.append({\"role\": \"system\", \"content\": system})\n",
    "    messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "\n",
    "    try:\n",
    "        kwargs = {\n",
    "            \"model\": model,\n",
    "            \"messages\": messages,\n",
    "        }\n",
    "        model_lower = model.lower().strip()\n",
    "\n",
    "        # Bedrock Anthropic models\n",
    "        if model_lower.startswith(\"anthropic/\") or model_lower.startswith(\"anthropic.\") or \"claude\" in model_lower or model_lower.startswith(\"amazon.\"):\n",
    "            # === Bedrock-style ===\n",
    "            if max_tokens:\n",
    "                kwargs[\"max_tokens\"] = max_tokens\n",
    "            if tools:\n",
    "                kwargs[\"tools\"] = tools\n",
    "        else:\n",
    "            # === OpenAI-style ===\n",
    "            if max_tokens:\n",
    "                kwargs[\"max_tokens\"] = max_tokens\n",
    "            if tools:\n",
    "                kwargs[\"tools\"] = tools\n",
    "                kwargs[\"tool_choice\"] = \"auto\"\n",
    "\n",
    "        completion = client.chat.completions.create(**kwargs)\n",
    "        text = _extract_text_from_completion(completion)\n",
    "\n",
    "        if use_cache:\n",
    "            _llm_cache[key] = text\n",
    "\n",
    "        if expect_json:\n",
    "            try:\n",
    "                return json.loads(text)\n",
    "            except Exception:\n",
    "                try:\n",
    "                    return _extract_json_from_text(text)\n",
    "                except Exception as e:\n",
    "                    return {\"raw\": text, \"parse_error\": str(e)}\n",
    "        return text\n",
    "\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"LLM call failed (model={model}): {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d522c32",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# logger.py\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "class GEPA_Logger:\n",
    "    def __init__(self, log_dir=\"logs\"):\n",
    "        Path(log_dir).mkdir(parents=True, exist_ok=True)\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        self.log_file = Path(log_dir) / f\"gepa_run_{timestamp}.jsonl\"\n",
    "\n",
    "    def log(self, record: dict):\n",
    "        \"\"\"Append one record as JSON line\"\"\"\n",
    "        with open(self.log_file, \"a\", encoding=\"utf-8\") as f:\n",
    "            f.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e049b65",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# mcp_server.py\n",
    "from flask import Flask, request, jsonify\n",
    "from neo4j import GraphDatabase\n",
    "import time\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# --- Neo4j connection ---\n",
    "URI = \"bolt://localhost:7687\"\n",
    "USER = \"neo4j\"\n",
    "PASSWORD = \"NHAN_TAI_DAT_VIET_098\"\n",
    "driver = GraphDatabase.driver(URI, auth=(USER, PASSWORD))\n",
    "\n",
    "# --- Cache dictionary ---\n",
    "cache = {}\n",
    "CACHE_EXPIRY = 10 * 60  # 10 minutes in seconds\n",
    "\n",
    "def cache_get(key):\n",
    "    entry = cache.get(key)\n",
    "    if entry:\n",
    "        value, timestamp = entry\n",
    "        if time.time() - timestamp < CACHE_EXPIRY:\n",
    "            return value\n",
    "    return None\n",
    "\n",
    "def cache_set(key, value):\n",
    "    cache[key] = (value, time.time())\n",
    "\n",
    "# --- Helper function to run a query ---\n",
    "def run_query(query, params=None):\n",
    "    with driver.session() as session:\n",
    "        result = session.run(query, params or {})\n",
    "        return [record.data() for record in result]\n",
    "\n",
    "# --- 1. Cached queries with expiry ---\n",
    "def get_passage_rules():\n",
    "    key = \"passage_rules\"\n",
    "    cached = cache_get(key)\n",
    "    if cached:\n",
    "        return cached\n",
    "    query = \"\"\"\n",
    "    MATCH (r:FormatRule)\n",
    "    RETURN r.id AS id, r.description AS description\n",
    "    ORDER BY r.id\n",
    "    \"\"\"\n",
    "    result = run_query(query)\n",
    "    cache_set(key, result)\n",
    "    return result\n",
    "\n",
    "def get_question_type_context(qtype_id):\n",
    "    key = f\"question_type_context:{qtype_id}\"\n",
    "    cached = cache_get(key)\n",
    "    if cached:\n",
    "        return cached\n",
    "    query = \"\"\"\n",
    "    MATCH (q:QuestionType {id: $qtype_id})-[:HAS_RULE]->(r:QuestionTypeRule)\n",
    "    RETURN r.id AS id, r.description AS description\n",
    "    \"\"\"\n",
    "    result = run_query(query, {\"qtype_id\": qtype_id})\n",
    "    cache_set(key, result)\n",
    "    return result\n",
    "\n",
    "def get_distractor_patterns():\n",
    "    key = \"distractor_patterns\"\n",
    "    cached = cache_get(key)\n",
    "    if cached:\n",
    "        return cached\n",
    "    query = \"MATCH (d:Distractor) RETURN d.id AS id, d.description AS description\"\n",
    "    result = run_query(query)\n",
    "    cache_set(key, result)\n",
    "    return result\n",
    "\n",
    "def get_penmanship_rules():\n",
    "    key = \"penmanship_rules\"\n",
    "    cached = cache_get(key)\n",
    "    if cached:\n",
    "        return cached\n",
    "    query = \"\"\"\n",
    "    MATCH (p:Penmanship)-[:HAS_SUBRULE]->(s:PenmanshipSubrule)\n",
    "    RETURN p.id AS penmanship_id, p.description AS penmanship_desc,\n",
    "           collect({id: s.id, description: s.description}) AS subrules\n",
    "    \"\"\"\n",
    "    result = run_query(query)\n",
    "    cache_set(key, result)\n",
    "    return result\n",
    "def get_passage_examples():\n",
    "    key = \"passage_examples\"\n",
    "    cached = cache_get(key)\n",
    "    if cached:\n",
    "        return cached\n",
    "    query = \"\"\"\n",
    "    MATCH (p:Entity {type: \"PassageExample\"})\n",
    "    RETURN p.id AS id, p.title AS title, p.passage AS passage\n",
    "    ORDER BY p.id\n",
    "    \"\"\"\n",
    "    result = run_query(query)\n",
    "    cache_set(key, result)\n",
    "    return result\n",
    "\n",
    "def get_question_examples(qtype_id=None):\n",
    "    if qtype_id:\n",
    "        query = \"\"\"\n",
    "        MATCH (q:Entity {type: \"QuestionExample\"})-[:OF_TYPE]->(qt:Entity {id: $qtype_id})\n",
    "        RETURN q.id AS id, q.question_text AS question_text, q.options AS options,\n",
    "               q.answer AS answer, q.rationale AS rationale, qt.id AS qtype_id\n",
    "        ORDER BY q.id\n",
    "        \"\"\"\n",
    "        return run_query(query, {\"qtype_id\": qtype_id})\n",
    "    else:\n",
    "        query = \"\"\"\n",
    "        MATCH (q:Entity {type: \"QuestionExample\"})-[:OF_TYPE]->(qt:Entity)\n",
    "        RETURN q.id AS id, q.question_text AS question_text, q.options AS options,\n",
    "               q.answer AS answer, q.rationale AS rationale, qt.id AS qtype_id\n",
    "        ORDER BY q.id\n",
    "        \"\"\"\n",
    "        return run_query(query)\n",
    "# --- 2. Flask routes ---\n",
    "@app.route(\"/get_passage_rules\", methods=[\"GET\"])\n",
    "def route_passage_rules():\n",
    "    return jsonify(get_passage_rules())\n",
    "\n",
    "@app.route(\"/get_question_type_context/<qtype_id>\", methods=[\"GET\"])\n",
    "def route_question_type_context(qtype_id):\n",
    "    return jsonify(get_question_type_context(qtype_id))\n",
    "\n",
    "@app.route(\"/get_distractor_patterns\", methods=[\"GET\"])\n",
    "def route_distractor_patterns():\n",
    "    return jsonify(get_distractor_patterns())\n",
    "\n",
    "@app.route(\"/get_penmanship_rules\", methods=[\"GET\"])\n",
    "def route_penmanship_rules():\n",
    "    return jsonify(get_penmanship_rules())\n",
    "# --- 3. Example endpoints (optional style guidance) ---\n",
    "@app.route(\"/get_passage_examples\", methods=[\"GET\"])\n",
    "def route_passage_examples():\n",
    "    return jsonify(get_passage_examples())\n",
    "\n",
    "@app.route(\"/get_question_examples\", methods=[\"GET\"])\n",
    "def route_question_examples_all():\n",
    "    return jsonify(get_question_examples())\n",
    "\n",
    "@app.route(\"/get_question_examples/<qtype_id>\", methods=[\"GET\"])\n",
    "def route_question_examples_by_type(qtype_id):\n",
    "    return jsonify(get_question_examples(qtype_id))\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(host=\"0.0.0.0\", port=8000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1dce5f3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# mcp_client.py\n",
    "import requests\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "class MCPClient:\n",
    "    def __init__(self, base_url: str = \"http://localhost:8000\"):\n",
    "        self.base_url = base_url.rstrip(\"/\")\n",
    "\n",
    "    def _get(self, path: str) -> Any:\n",
    "        resp = requests.get(f\"{self.base_url}{path}\")\n",
    "        resp.raise_for_status()\n",
    "        return resp.json()\n",
    "\n",
    "    def get_passage_rules(self) -> List[Dict]:\n",
    "        return self._get(\"/get_passage_rules\")\n",
    "\n",
    "    def get_question_type_context(self, qtype_id: str) -> Dict:\n",
    "        return self._get(f\"/get_question_type_context/{qtype_id}\")\n",
    "\n",
    "    def get_distractor_patterns(self) -> List[Dict]:\n",
    "        return self._get(\"/get_distractor_patterns\")\n",
    "\n",
    "    def get_penmanship_rules(self) -> List[Dict]:\n",
    "        return self._get(\"/get_penmanship_rules\")\n",
    "    def get_passage_examples(self) -> List[Dict]:\n",
    "        return self._get(\"/get_passage_examples\")\n",
    "\n",
    "    def get_question_examples(self, qtype_id: str = None) -> List[Dict]:\n",
    "        if qtype_id:\n",
    "            return self._get(f\"/get_question_examples/{qtype_id}\")\n",
    "        return self._get(\"/get_question_examples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2ea928",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# retriever.py\n",
    "import requests\n",
    "import re\n",
    "from googlesearch import search   # your existing web-search tool\n",
    "from llm import call_llm\n",
    "SEMANTIC_SCHOLAR_URL = \"https://api.semanticscholar.org/graph/v1/paper/search\"\n",
    "OPENALEX_URL = \"https://api.openalex.org/works\"\n",
    "CROSSREF_URL = \"https://api.crossref.org/works\"\n",
    "\n",
    "\n",
    "def fetch_from_semantic_scholar(topic, limit=3):\n",
    "    params = {\n",
    "        \"query\": topic,\n",
    "        \"limit\": limit,\n",
    "        \"fields\": \"title,abstract,year,url\"\n",
    "    }\n",
    "    try:\n",
    "        resp = requests.get(SEMANTIC_SCHOLAR_URL, params=params, timeout=10)\n",
    "        resp.raise_for_status()\n",
    "        data = resp.json()\n",
    "    except Exception as e:\n",
    "        print(\"Semantic Scholar fetch failed:\", e)\n",
    "        return []\n",
    "\n",
    "    results = []\n",
    "    for i, paper in enumerate(data.get(\"data\", []), 1):\n",
    "        title = paper.get(\"title\", \"Untitled\")\n",
    "        year = paper.get(\"year\")\n",
    "        abstract = paper.get(\"abstract\") or title\n",
    "        url = paper.get(\"url\", \"\")\n",
    "        facts = []\n",
    "        if year:\n",
    "            facts.append(f\"Year: {year}\")\n",
    "        if url:\n",
    "            facts.append(f\"Source: {url}\")\n",
    "        results.append(normalize_source(\"S\", i, title, abstract=abstract, facts=facts, url=url))\n",
    "    return results\n",
    "\n",
    "\n",
    "def fetch_from_openalex(topic, limit=3):\n",
    "    params = {\"search\": topic, \"per-page\": limit}\n",
    "    try:\n",
    "        resp = requests.get(OPENALEX_URL, params=params, timeout=10)\n",
    "        resp.raise_for_status()\n",
    "        data = resp.json()\n",
    "    except Exception as e:\n",
    "        print(\"OpenAlex fetch failed:\", e)\n",
    "        return []\n",
    "\n",
    "    results = []\n",
    "    for i, work in enumerate(data.get(\"results\", []), 1):\n",
    "        title = work.get(\"title\", \"Untitled\")\n",
    "        year = work.get(\"publication_year\", \"Unknown\")\n",
    "        doi = work.get(\"doi\", \"\")\n",
    "        abstract = work.get(\"abstract\", \"\") or title\n",
    "        facts = []\n",
    "        if year and year != \"Unknown\":\n",
    "            facts.append(f\"Year: {year}\")\n",
    "        if doi:\n",
    "            facts.append(f\"DOI: {doi}\")\n",
    "        results.append(normalize_source(\"O\", i, title, abstract=abstract, facts=facts))\n",
    "    return results\n",
    "\n",
    "\n",
    "def fetch_from_crossref(topic, limit=3):\n",
    "    params = {\"query\": topic, \"rows\": limit}\n",
    "    try:\n",
    "        resp = requests.get(CROSSREF_URL, params=params, timeout=10)\n",
    "        resp.raise_for_status()\n",
    "        data = resp.json()\n",
    "    except Exception as e:\n",
    "        print(\"CrossRef fetch failed:\", e)\n",
    "        return []\n",
    "\n",
    "    results = []\n",
    "    for i, item in enumerate(data.get(\"message\", {}).get(\"items\", []), 1):\n",
    "        title = \" \".join(item.get(\"title\", [])) or \"Untitled\"\n",
    "        year = item.get(\"issued\", {}).get(\"date-parts\", [[None]])[0][0]\n",
    "        doi = item.get(\"DOI\", \"\")\n",
    "        facts = []\n",
    "        if year:\n",
    "            facts.append(f\"Year: {year}\")\n",
    "        if doi:\n",
    "            facts.append(f\"DOI: {doi}\")\n",
    "        results.append(normalize_source(\"C\", i, title, abstract=title, facts=facts))\n",
    "    return results\n",
    "\n",
    "def fetch_from_web(topic, limit=3):\n",
    "    try:\n",
    "        results = list(search(f\"{topic} research facts experiments events\"))  # convert to list\n",
    "    except Exception as e:\n",
    "        print(\"Web search failed:\", e)\n",
    "        return []\n",
    "\n",
    "    sources = []\n",
    "    for i, r in enumerate(results[:limit], 1):\n",
    "        # depending on your googlesearch version, r might be a string (URL) not dict\n",
    "        if isinstance(r, str):\n",
    "            title = r\n",
    "            snippet = \"\"\n",
    "            url = r\n",
    "        else:\n",
    "            title = r.get(\"title\", \"Untitled\")\n",
    "            snippet = r.get(\"snippet\", \"\")\n",
    "            url = r.get(\"url\", \"\")\n",
    "\n",
    "        abstract = snippet or title\n",
    "        facts = []\n",
    "        if url:\n",
    "            facts.append(f\"Source: {url}\")\n",
    "        sources.append(normalize_source(\"W\", i, title, abstract=abstract, facts=facts, url=url))\n",
    "    return sources\n",
    "\n",
    "\n",
    "def highlight_facts(text):\n",
    "    \"\"\"\n",
    "    Extract and emphasize numbers, years, percentages, and sample sizes \n",
    "    so the LLM uses them more reliably in passages.\n",
    "    \"\"\"\n",
    "    # Bold numbers and years in brackets (so they stand out in the prompt)\n",
    "    text = re.sub(r\"(\\b\\d{4}\\b)\", r\"<YEAR:\\1>\", text)        # years\n",
    "    text = re.sub(r\"(\\b\\d+%|\\b\\d+\\.\\d+%|\\b\\d+\\b)\", r\"<NUM:\\1>\", text)  # numbers/percentages\n",
    "    return text\n",
    "\n",
    "\n",
    "def process_sources(sources: list[dict]) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Apply highlighting to abstracts of all sources.\n",
    "    \"\"\"\n",
    "    processed = []\n",
    "    for s in sources:\n",
    "        s_copy = dict(s)  # shallow copy\n",
    "        if \"abstract\" in s_copy and s_copy[\"abstract\"]:\n",
    "            s_copy[\"abstract\"] = highlight_facts(s_copy[\"abstract\"])\n",
    "        processed.append(s_copy)\n",
    "    return processed\n",
    "\n",
    "\n",
    "def retrieve_sources(topic: str, limit: int = 5):\n",
    "    \"\"\"\n",
    "    Use LLM to suggest academic-style sources for a topic.\n",
    "    Returns a list of dicts with id, abstract, facts.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    You are an assistant that finds academic references.\n",
    "\n",
    "    Task: List {limit} academic papers about the topic \"{topic}\".\n",
    "    For each paper, return:\n",
    "      - title\n",
    "      - year\n",
    "      - abstract (1–3 sentences)\n",
    "      - optional URL if known\n",
    "\n",
    "    Output format: JSON array of objects with keys:\n",
    "      \"title\", \"year\", \"abstract\", \"url\"\n",
    "    \"\"\"\n",
    "\n",
    "    resp = call_llm(prompt, expect_json=True)\n",
    "\n",
    "    sources = []\n",
    "    if isinstance(resp, list):\n",
    "        for i, item in enumerate(resp[:limit], 1):\n",
    "            title = item.get(\"title\", \"Untitled\")\n",
    "            year = item.get(\"year\", \"\")\n",
    "            abstract = item.get(\"abstract\", title)\n",
    "            url = item.get(\"url\", \"\")\n",
    "\n",
    "            facts = []\n",
    "            if year:\n",
    "                facts.append(f\"Year: {year}\")\n",
    "            if url:\n",
    "                facts.append(f\"Source: {url}\")\n",
    "\n",
    "            sources.append({\n",
    "                \"id\": f\"L{i}\",\n",
    "                \"abstract\": abstract,\n",
    "                \"facts\": facts\n",
    "            })\n",
    "    else:\n",
    "        # fallback: treat raw text as one abstract\n",
    "        sources = [{\n",
    "            \"id\": \"L1\",\n",
    "            \"abstract\": str(resp),\n",
    "            \"facts\": []\n",
    "        }]\n",
    "\n",
    "    sources = deduplicate_sources(sources)\n",
    "    return sources\n",
    "\n",
    "\n",
    "def normalize_source(id_prefix: str, idx: int, title: str,\n",
    "                     abstract: str | None = None,\n",
    "                     facts: list[str] | None = None,\n",
    "                     url: str | None = None) -> dict:\n",
    "    \"\"\"\n",
    "    Normalize source into {id, abstract, facts}.\n",
    "    - abstract: main text body\n",
    "    - facts: list of short factual strings (years, %s, DOIs, URLs, etc.)\n",
    "    \"\"\"\n",
    "    facts = facts or []\n",
    "    if url:\n",
    "        facts.append(f\"Source: {url}\")\n",
    "    return {\n",
    "        \"id\": f\"{id_prefix}{idx}\",\n",
    "        \"abstract\": abstract or title,\n",
    "        \"facts\": facts\n",
    "    }\n",
    "\n",
    "\n",
    "def deduplicate_sources(sources: list[dict]) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Deduplicate sources by abstract text (case-insensitive).\n",
    "    \"\"\"\n",
    "    seen = set()\n",
    "    unique = []\n",
    "    for s in sources:\n",
    "        key = s.get(\"abstract\") or s.get(\"text\", \"\")\n",
    "        key = key.lower().strip()\n",
    "        if key and key not in seen:\n",
    "            seen.add(key)\n",
    "            unique.append(s)\n",
    "    return unique\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2650db5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# validators.py\n",
    "\"\"\"\n",
    "Validators for GEPA (µ_f).\n",
    "Extended with:\n",
    " - Penmanship scoring (from KG rules)\n",
    " - Writing style & cohesion checks\n",
    " - Distractor quality validation\n",
    " - Weighted composite score → IELTS band\n",
    " - Feedback example builder (from old feedback.py)\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "import json\n",
    "from typing import Tuple, Dict, Any, List\n",
    "from config import LLM_API_KEY, OPENAI_BASE_URL\n",
    "from json_strict import safe_json_loads\n",
    "# ---------- Utilities ----------\n",
    "def clean_passage_body(passage_text: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove metadata lines like 'Quiz title:', 'Quiz description:', etc.\n",
    "    Keep only the labeled paragraphs and summary.\n",
    "    \"\"\"\n",
    "    lines = passage_text.strip().splitlines()\n",
    "    body_lines = [ln for ln in lines if ln.strip().startswith(\"Text:\") or ln.strip().startswith(\"Summary:\")]\n",
    "    return \"\\n\".join(body_lines)\n",
    "\n",
    "\n",
    "def word_count(text: str) -> int:\n",
    "    # Count words in cleaned passage body only\n",
    "    body = clean_passage_body(text)\n",
    "    return len(re.findall(r\"\\b\\w+\\b\", body))\n",
    "\n",
    "\n",
    "def paragraph_count(text: str) -> int:\n",
    "    \"\"\"\n",
    "    Count how many paragraphs exist based on 'Text: [A-Z].' markers.\n",
    "    Example: 'Text: A.' → counts as one paragraph.\n",
    "    \"\"\"\n",
    "    body = clean_passage_body(text)\n",
    "    matches = re.findall(r\"Text:\\s*[A-Z]\\.\", body)\n",
    "    return len(matches)\n",
    "\n",
    "\n",
    "def validate_passage_text(passage_text: str) -> Tuple[float, List[str], List[str]]:\n",
    "    raw_traces = []\n",
    "    fb_traces = []\n",
    "\n",
    "    wc = word_count(passage_text)\n",
    "    pc = paragraph_count(passage_text)\n",
    "\n",
    "    # --- Word count scoring ---\n",
    "    ideal = 900\n",
    "    width = 900  # wider tolerance\n",
    "    wc_score = max(0.0, 1 - abs(wc - ideal) / width)\n",
    "    raw_traces.append(f\"word_count={wc}\")\n",
    "\n",
    "    if wc < 600:\n",
    "        fb_traces.append(f\"Passage too short ({wc} words). Aim for ~900 words.\")\n",
    "    elif wc > 1200:\n",
    "        fb_traces.append(f\"Passage too long ({wc} words). Aim for ~900 words.\")\n",
    "    else:\n",
    "        fb_traces.append(f\"Passage length acceptable ({wc} words).\")\n",
    "\n",
    "    # --- Paragraph count ---\n",
    "    if 5 <= pc <= 9:\n",
    "        pc_score = 1.0\n",
    "    else:\n",
    "        pc_score = max(0.0, 1 - abs(pc - 7) / 7)\n",
    "    raw_traces.append(f\"paragraph_count={pc}\")\n",
    "\n",
    "    if pc < 5:\n",
    "        fb_traces.append(f\"Too few paragraphs ({pc}). Target 5–9.\")\n",
    "    elif pc > 9:\n",
    "        fb_traces.append(f\"Too many paragraphs ({pc}). Target 5–9.\")\n",
    "    else:\n",
    "        fb_traces.append(f\"Paragraph count within range ({pc}).\")\n",
    "\n",
    "    # --- Summary line ---\n",
    "    if \"Summary:\" in passage_text:\n",
    "        sum_score = 1.0\n",
    "        raw_traces.append(\"summary=present\")\n",
    "        fb_traces.append(\"Summary line present at end.\")\n",
    "    else:\n",
    "        sum_score = 0.5   # softer penalty\n",
    "        raw_traces.append(\"summary=missing\")\n",
    "        fb_traces.append(\"Missing required summary line at end.\")\n",
    "\n",
    "    score = 0.5 * wc_score + 0.3 * pc_score + 0.2 * sum_score\n",
    "    return score, raw_traces, fb_traces\n",
    "\n",
    "# ---------- Question validator ----------\n",
    "def validate_questions_structure(questions_list) -> Tuple[float, List[str], List[str]]:\n",
    "    raw_traces = []\n",
    "    fb_traces = []\n",
    "\n",
    "    if not isinstance(questions_list, list) or not questions_list:\n",
    "        return 0.3, [\"questions=missing_or_not_list\"], [\n",
    "            \"Questions missing or invalid JSON. Require a valid JSON array of questions.\"\n",
    "        ]\n",
    "\n",
    "    total_q = len(questions_list)\n",
    "    ok_count = 0\n",
    "    for q in questions_list:\n",
    "        if not q.get(\"id\") or not q.get(\"question_text\"):\n",
    "            raw_traces.append(f\"question_missing_fields:{q.get('id','?')}\")\n",
    "            fb_traces.append(\"Some questions missing ID or text → ensure each has 'id' and 'question_text'.\")\n",
    "            continue\n",
    "        if \"answer\" not in q or q[\"answer\"] is None:\n",
    "            raw_traces.append(f\"question_{q.get('id')} missing_answer\")\n",
    "            fb_traces.append(f\"Question {q.get('id','?')} missing answer → always include 'answer'.\")\n",
    "            continue\n",
    "        ok_count += 1\n",
    "\n",
    "    score = ok_count / total_q if total_q else 0.3\n",
    "    if score < 1.0:\n",
    "        fb_traces.append(f\"Only {ok_count}/{total_q} questions valid. Ensure all have complete fields.\")\n",
    "    else:\n",
    "        fb_traces.append(\"All questions valid and well-structured.\")\n",
    "\n",
    "    return score, raw_traces, fb_traces\n",
    "\n",
    "\n",
    "# ---------- Extractive check ----------\n",
    "def extractive_answer_check(passage_text: str, question) -> Tuple[float, str]:\n",
    "    ans = question.get(\"answer\", \"\")\n",
    "    if not ans:\n",
    "        return 0.0, \"answer_empty\"\n",
    "    ans_lower = ans.lower()\n",
    "    if ans_lower in passage_text.lower():\n",
    "        return 1.0, \"answer_span_found\"\n",
    "    words = [w for w in re.findall(r\"\\w+\", ans_lower) if len(w) > 3]\n",
    "    if words and all(w in passage_text.lower() for w in words):\n",
    "        return 0.75, \"answer_words_all_present\"\n",
    "    return 0.0, \"answer_missing_or_paraphrased\"\n",
    "\n",
    "\n",
    "# ---------- Penmanship validator ----------\n",
    "def validate_penmanship(text: str, rules: List[Dict] | None = None) -> Tuple[float, List[str], List[str]]:\n",
    "    raw_traces = []\n",
    "    fb_traces = []\n",
    "\n",
    "    if not rules:\n",
    "        return 1.0, [\"penmanship=skipped(no_rules)\"], [\"No penmanship rules provided.\"]\n",
    "\n",
    "    violations = []\n",
    "    for rule in rules:\n",
    "        desc = rule.get(\"description\", \"\")\n",
    "        patterns = rule.get(\"banned_patterns\", [])\n",
    "        for pat in patterns:\n",
    "            if re.search(pat, text):\n",
    "                violations.append(desc)\n",
    "                raw_traces.append(f\"penmanship_violation:{desc}\")\n",
    "                fb_traces.append(f\"Penmanship violation: {desc}\")\n",
    "\n",
    "    score = 1.0 if not violations else max(0.0, 1 - len(violations) / len(rules))\n",
    "    if not violations:\n",
    "        fb_traces.append(\"No penmanship violations detected.\")\n",
    "    return score, raw_traces, fb_traces\n",
    "\n",
    "\n",
    "# ---------- Distractor quality validator ----------\n",
    "def validate_distractors(questions: list) -> Tuple[float, List[str], List[str]]:\n",
    "    raw_traces = []\n",
    "    fb_traces = []\n",
    "\n",
    "    if not questions:\n",
    "        return 0.0, [\"distractors=missing\"], [\"No distractors found. Require at least 2–3 per question.\"]\n",
    "\n",
    "    valid = 0\n",
    "    total = 0\n",
    "    for q in questions:\n",
    "        opts = q.get(\"options\", [])\n",
    "        ans = q.get(\"answer\", \"\")\n",
    "        for opt in opts:\n",
    "            if opt == ans:\n",
    "                continue\n",
    "            total += 1\n",
    "            if abs(len(opt) - len(ans)) < 10:  # rough similarity\n",
    "                valid += 1\n",
    "            else:\n",
    "                raw_traces.append(f\"distractor_bad_length:{opt}\")\n",
    "                fb_traces.append(f\"One distractor too different in length: '{opt}'\")\n",
    "\n",
    "    score = valid / total if total else 0.0\n",
    "    if score >= 0.7:\n",
    "        fb_traces.append(f\"Distractor quality good ({valid}/{total} acceptable).\")\n",
    "    else:\n",
    "        fb_traces.append(f\"Distractor quality weak ({valid}/{total} acceptable). Balance lengths with correct answer.\")\n",
    "\n",
    "    return score, raw_traces, fb_traces\n",
    "\n",
    "\n",
    "\n",
    "# ---------- Band mapping ----------\n",
    "def to_band(score_01: float) -> float:\n",
    "    band = score_01 * 9.0\n",
    "    return round(band * 2) / 2\n",
    "\n",
    "def validate_by_llm(passage: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Call LLM to evaluate passage similarity with IELTS style.\n",
    "    Returns dict with category scores + overall + feedback.\n",
    "    \"\"\"\n",
    "    from openai import OpenAI\n",
    "    import json\n",
    "\n",
    "    client = OpenAI(api_key=LLM_API_KEY, base_url=OPENAI_BASE_URL)\n",
    "\n",
    "    IELTS_EVAL_PROMPT = \"\"\"\n",
    "    You are an IELTS Reading examiner assistant.\n",
    "    Evaluate the following passage according to IELTS Reading passage similarity.\n",
    "    Score each category from 0–5.\n",
    "    Then give an overall similarity score (0–100).\n",
    "    Finally, provide feedback in 2–3 sentences.\n",
    "\n",
    "    Categories:\n",
    "    - Language & Style (formal, academic tone, appropriate vocabulary)\n",
    "    - Clarity & Accessibility (complex but understandable, not excessively dense)\n",
    "    - Content & Relevance (topics typical of IELTS, like science, culture, history, environment)\n",
    "    - Balance of Abstract vs Concrete (uses examples/data in addition to theory)\n",
    "\n",
    "    Return JSON with keys:\n",
    "    length_structure, language_style, clarity_accessibility, content_relevance, balance, overall, feedback.\n",
    "\n",
    "    Passage:\n",
    "    \\\"\\\"\\\"{passage}\\\"\\\"\\\"\n",
    "    \"\"\".format(passage=passage)\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-5\",\n",
    "        messages=[{\"role\": \"developer\", \"content\": \"You are an IELTS Reading examiner assistant.\"},\n",
    "                  {\"role\": \"user\", \"content\": IELTS_EVAL_PROMPT}],\n",
    "        temperature=0.0\n",
    "    )\n",
    "\n",
    "    raw = response.choices[0].message.content.strip()\n",
    "    parsed = safe_json_loads(raw)\n",
    "    if isinstance(parsed, dict) and parsed.get(\"__parse_error\"):\n",
    "        # keep old fallback numbers (existing behavior)\n",
    "        result = {\n",
    "            \"language_style\": 0,\n",
    "            \"clarity_accessibility\": 0,\n",
    "            \"content_relevance\": 0,\n",
    "            \"balance\": 0,\n",
    "            \"overall\": 50,\n",
    "        }\n",
    "    else:\n",
    "        result = parsed\n",
    "    return result\n",
    "\n",
    "\n",
    "# ---------- Composer ----------\n",
    "def score_passage_and_questions(outputs: Dict[str, Any], topic: str,\n",
    "                                penmanship_rules: List[Dict] | None = None) -> Tuple[Dict[str, float], Dict[str, List[str]]]:\n",
    "    raw_traces = []\n",
    "    fb_traces = []\n",
    "    passage = outputs.get(\"passage\", \"\")\n",
    "    questions_raw = outputs.get(\"questions\", \"\")\n",
    "\n",
    "    # Parse questions if JSON string\n",
    "    questions = questions_raw\n",
    "    if isinstance(questions_raw, str):\n",
    "        try:\n",
    "            m = re.search(r'(\\{.*\\}|\\[.*\\])', questions_raw, flags=re.S)\n",
    "            if m:\n",
    "                parsed = safe_json_loads(m.group(1))\n",
    "                if isinstance(parsed, dict) and parsed.get(\"__parse_error\"):\n",
    "                    # handle as failure (previous logic might have continued)\n",
    "                    questions = None  # or fallback behavior as your code expects\n",
    "                else:\n",
    "                    questions = parsed\n",
    "            else:\n",
    "                questions = None\n",
    "\n",
    "        except Exception:\n",
    "            questions = []\n",
    "\n",
    "    # --- Sub-scores with extended validators ---\n",
    "    p_score, p_raw, p_fb = validate_passage_text(passage)\n",
    "    q_score, q_raw, q_fb = validate_questions_structure(questions)\n",
    "    penmanship_score, pn_raw, pn_fb = validate_penmanship(passage, penmanship_rules)\n",
    "    distractor_score, d_raw, d_fb = validate_distractors(questions)\n",
    "    # --- New LLM validator ---\n",
    "    llm_scores = validate_by_llm(passage)\n",
    "\n",
    "    # --- Collect raw traces (debug) ---\n",
    "    raw_traces += [f\"P:{t}\" for t in p_raw]\n",
    "    raw_traces += [f\"Q:{t}\" for t in q_raw]\n",
    "    # raw_traces += [f\"PN:{t}\" for t in pn_raw]\n",
    "    # raw_traces += [f\"D:{t}\" for t in d_raw]\n",
    "    raw_traces.append(f\"LLM:{llm_scores}\")\n",
    "\n",
    "    # --- Collect feedback traces (for GEPA mutation) ---\n",
    "    fb_traces += [f\"P:{t}\" for t in p_fb]\n",
    "    fb_traces += [f\"Q:{t}\" for t in q_fb]\n",
    "    # fb_traces += [f\"PN:{t}\" for t in pn_fb]\n",
    "    # fb_traces += [f\"D:{t}\" for t in d_fb]\n",
    "    fb_traces.append(f\"LLM feedback: {llm_scores.get('feedback','')}\")\n",
    "    # --- Extractive check ---\n",
    "    extract_scores = []\n",
    "    for q in questions:\n",
    "        s, trace = extractive_answer_check(passage, q)\n",
    "        extract_scores.append(s)\n",
    "        raw_traces.append(f\"EX:{q.get('id','?')}:{trace}\")\n",
    "        fb_traces.append(f\"Answer validation for {q.get('id','?')}: {trace}\")\n",
    "    extract_avg = sum(extract_scores) / len(extract_scores) if extract_scores else 0.0\n",
    "\n",
    "    # --- Scores dict ---\n",
    "    scores = {\n",
    "        \"passage\": p_score,\n",
    "        \"questions\": q_score,\n",
    "        # \"distractors\": distractor_score,\n",
    "        \"extractive\": extract_avg,\n",
    "        \"language_style\": llm_scores[\"language_style\"],\n",
    "        \"clarity_accessibility\": llm_scores[\"clarity_accessibility\"],\n",
    "        \"content_relevance\": llm_scores[\"content_relevance\"],\n",
    "        \"balance\": llm_scores[\"balance\"],\n",
    "    }\n",
    "\n",
    "    # --- Final weighted score ---\n",
    "    final_score = (\n",
    "        0.20 * p_score +\n",
    "        0.15 * q_score +\n",
    "        0.05 * extract_avg +\n",
    "        0.15 * llm_scores[\"language_style\"] +\n",
    "        0.20 * llm_scores[\"clarity_accessibility\"] + \n",
    "        0.05 * llm_scores[\"content_relevance\"] + \n",
    "        0.20 * llm_scores[\"balance\"]\n",
    "        # + 0.10 * distractor_score  (enable later if distractors are required)\n",
    "    )\n",
    "    band = to_band(final_score)\n",
    "\n",
    "    raw_traces.append(f\"SCORE_BAND={band}\")\n",
    "    fb_traces.append(f\"Overall estimated IELTS band: {band} (0–9 scale).\")\n",
    "    scores = {k: float(v) for k, v in scores.items()}\n",
    "    return scores, {\"raw\": raw_traces, \"feedback\": fb_traces}\n",
    "\n",
    "\n",
    "\n",
    "# ---------- Feedback Examples ----------\n",
    "def build_feedback_examples(topic: str, passage: str, issues: List[str]) -> List[Dict[str, str]]:\n",
    "    return [{\n",
    "        \"input\": topic,\n",
    "        \"output\": passage[:200],\n",
    "        \"feedback\": \"; \".join(issues) if issues else \"Looks good.\"\n",
    "    }]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82975273",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# executors.py\n",
    "\"\"\"\n",
    "Upgraded executors:\n",
    " - Uses MCPClient to fetch KG constraints (cached MCP endpoints).\n",
    " - Strict prompt templates with constraints.\n",
    " - Validate + retry loop using validators.py.\n",
    " - Backwards-compatible executor signatures used by gepa_driver / main.py:\n",
    "     executor(prompt_template, topic, outputs_so_far) -> returns str or structured object\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import logging\n",
    "import time\n",
    "from typing import Any, Dict, List, Optional\n",
    "from json_strict import safe_json_loads\n",
    "from mcp_client import MCPClient\n",
    "from llm import _extract_json_from_text\n",
    "# Try to import your llm helper (project-specific). If not present, we'll raise helpful errors.\n",
    "try:\n",
    "    from llm import call_llm, DEBUG_STUB  # call_llm(prompt, expect_json=False, max_tokens=...)\n",
    "except Exception:\n",
    "    # Minimal fallback placeholder to avoid crashes during static analysis.\n",
    "    DEBUG_STUB = True\n",
    "    def call_llm(prompt: str, expect_json: bool = False, max_tokens: int = 10000):\n",
    "        raise RuntimeError(\"llm.call_llm not available. Please provide llm.call_llm in your project.\")\n",
    "\n",
    "# Import validators provided in your repo\n",
    "try:\n",
    "    from validators import validate_passage_text, validate_questions_structure\n",
    "except Exception:\n",
    "    # If validators.py is missing, create a minimal fallback so errors are clear.\n",
    "    def validate_passage_text(txt):\n",
    "        return 0.0, [\"validators.validate_passage_text unavailable\"]\n",
    "    def validate_questions_structure(qs):\n",
    "        return 0.0, [\"validators.validate_questions_structure unavailable\"]\n",
    "\n",
    "# configure logger\n",
    "logger = logging.getLogger(\"executors\")\n",
    "logger.setLevel(logging.DEBUG)\n",
    "handler = logging.StreamHandler()\n",
    "handler.setFormatter(logging.Formatter(\"[%(levelname)s] %(message)s\"))\n",
    "logger.addHandler(handler)\n",
    "\n",
    "# Instantiate MCP client (adjust URL if your MCP runs elsewhere)\n",
    "MCP = MCPClient(\"http://localhost:8000\")\n",
    "\n",
    "# -----------------------\n",
    "EXAMPLE_TOOLS = [\n",
    "  {\n",
    "    \"name\": \"fetch_passage_examples\",\n",
    "    \"description\": \"Retrieve example IELTS reading passages from the KG\",\n",
    "    \"parameters\": {\n",
    "      \"type\": \"object\",\n",
    "      \"properties\": {\n",
    "        \"test_id\": {\n",
    "          \"type\": \"string\",\n",
    "          \"description\": \"The ID of the test node, e.g., 'IELTS_Academic_Reading'\"\n",
    "        }\n",
    "      },\n",
    "      \"required\": [\"test_id\"]\n",
    "    }\n",
    "  },\n",
    "  {\n",
    "    \"name\": \"fetch_question_examples\",\n",
    "    \"description\": \"Retrieve example IELTS reading questions from the KG\",\n",
    "    \"parameters\": {\n",
    "      \"type\": \"object\",\n",
    "      \"properties\": {\n",
    "        \"test_id\": {\n",
    "          \"type\": \"string\",\n",
    "          \"description\": \"The ID of the test node, e.g., 'IELTS_Academic_Reading'\"\n",
    "        },\n",
    "        \"qtype_id\": {\n",
    "          \"type\": \"string\",\n",
    "          \"description\": \"Optional. The ID of the question type to filter by, e.g., 'Multiple_Choice'\"\n",
    "        }\n",
    "      },\n",
    "      \"required\": [\"test_id\"]\n",
    "    }\n",
    "  }\n",
    "]\n",
    "\n",
    "PASSAGE_TOOLS = [EXAMPLE_TOOLS[0]]    # passage examples\n",
    "QUESTION_TOOLS = [EXAMPLE_TOOLS[1]]   # question examples\n",
    "\n",
    "\n",
    "def maybe_call_examples(model_out: str, tool_group: str, qtype_id: Optional[str] = None):\n",
    "    \"\"\"\n",
    "    If the model requests example tool, fetch from MCP and continue generation.\n",
    "    Otherwise return the original output.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        parsed = safe_json_loads(model_out)\n",
    "        if isinstance(parsed, dict) and \"tool\" in parsed:\n",
    "            if parsed[\"tool\"] == \"get_passage_examples\":\n",
    "                examples = MCP._get(\"/get_passage_examples\")\n",
    "                followup = f\"Here are example passages:\\n{json.dumps(examples, indent=2)}\\n\\nNow continue your task.\"\n",
    "                return call_llm(followup, max_tokens=2500)\n",
    "            elif parsed[\"tool\"] == \"get_question_examples\":\n",
    "                endpoint = f\"/get_question_examples/{qtype_id}\"\n",
    "                examples = MCP._get(endpoint)\n",
    "                followup = f\"Here are example questions:\\n{json.dumps(examples, indent=2)}\\n\\nNow continue your task.\"\n",
    "                return call_llm(followup, max_tokens=2500)\n",
    "    except Exception:\n",
    "        return model_out\n",
    "    return model_out\n",
    "# -----------------------\n",
    "# Helpers\n",
    "# -----------------------\n",
    "def _strip_code_fence(text: str) -> str:\n",
    "    # remove triple-backtick fenced blocks if the model wrapped JSON in them\n",
    "    if text.strip().startswith(\"```\"):\n",
    "        parts = text.strip().split(\"```\", 2)\n",
    "        # If there are 3 parts, the middle is the fenced content\n",
    "        if len(parts) >= 3:\n",
    "            return parts[1].strip()\n",
    "    return text\n",
    "\n",
    "def _parse_json_from_model(text: str) -> Any:\n",
    "    \"\"\"\n",
    "    Try to robustly parse JSON returned by LLM.\n",
    "    - strips code fences\n",
    "    - tries json.loads\n",
    "    - if fails, attempts to find first '{' or '[' and parse substring\n",
    "    \"\"\"\n",
    "    txt = _strip_code_fence(text).strip()\n",
    "    parsed = safe_json_loads(txt)\n",
    "    if isinstance(parsed, dict) and parsed.get(\"__parse_error\"):\n",
    "        # attempt to find first JSON-like substring\n",
    "        first = min(\n",
    "            (txt.find(\"[\"), txt.find(\"{\"))\n",
    "            if \"[\" in txt or \"{\" in txt\n",
    "            else (len(txt), len(txt))\n",
    "        )\n",
    "        if first != -1 and first < len(txt):\n",
    "            sub = txt[first:]\n",
    "            try:\n",
    "                nw_parsed = safe_json_loads(sub)\n",
    "                if isinstance(nw_parsed, dict) and nw_parsed.get(\"__parse_error\"):\n",
    "                    try:\n",
    "                        return json.loads(sub)  # final direct attempt\n",
    "                    except Exception as e:\n",
    "                        logger.debug(\"JSON sub-parse failed: %s\", e)\n",
    "                        return {\"raw\": sub, \"parse_error\": nw_parsed[\"__parse_error\"]}\n",
    "                return nw_parsed\n",
    "            except Exception as e:\n",
    "                logger.debug(\"JSON salvage failed: %s\", e)\n",
    "                return {\"raw\": txt, \"parse_error\": parsed[\"__parse_error\"]}\n",
    "        else:\n",
    "            return {\"raw\": txt, \"parse_error\": parsed[\"__parse_error\"]}\n",
    "    else:\n",
    "        return parsed\n",
    "\n",
    "\n",
    "def _short(x: str, n: int = 400) -> str:\n",
    "    return x if len(x) <= n else x[:n] + \" ...\"\n",
    "\n",
    "# -----------------------\n",
    "# Prompt templates (strict)\n",
    "# -----------------------\n",
    "_PASSAGE_TEMPLATE = \"\"\"SYSTEM: You are an IELTS Academic Reading passage writer.\n",
    "TOPIC: {topic}\n",
    "\n",
    "CONTEXT / SOURCES: Review at least 10 real articles and draw inspiration from at least 15 official IELTS Reading practice tests. Select the most valuable sources and synthesize an original article with IELTS-style tone and structure.\n",
    "\n",
    "TASK (strict) FOLLOW THESE REQUIREMENTS:\n",
    "1) Produce ONE IELTS Academic Reading passage about the topic.\n",
    "2) Length: **800–1000** words (ideal ~900), divided into 6–8 paragraphs.\n",
    "3) Style: academic, formal, multiple paragraphs. Avoid being conversational.\n",
    "4) Use formal academic English (Band 6–7), and include 2–3 academic-level vocabulary items per paragraph but not too much and not too densely complicated.\n",
    "5) At least one paragraph should contain quantitative data or dates.\n",
    "6) IELTS passages are challenging, but they usually balance dense academic phrasing with clearer explanatory sections.\n",
    "7) IELTS passages often illustrate abstract ideas with case studies.\n",
    "8) Label the paragraphs from A onward (Paragraph A, Paragraph B, etc.)\n",
    "9) Include a final line starting exactly: Summary: <one-line summary>\n",
    "10) Do NOT include answers or questions in this output.\n",
    "11) Output ONLY the passage text (including the Summary line) — no JSON wrapper.\n",
    "\n",
    "The passage should be structured as follows:\n",
    "\n",
    "\"Quiz title: ...\\n\"\n",
    "\"Quiz description: ...\\n\\n\"\n",
    "\"Text title: Reading Passage\\n\"\n",
    "\"Text: A. <text with advanced vocabulary and complex structures (>=90 words)>\\n\"\n",
    "\"Text: B. <text with advanced vocabulary and complex structures (>=90 words)>\\n\"\n",
    "\"Text: C. <text with advanced vocabulary and complex structures (>=90 words)>\\n\"\n",
    "\"Text: D. <text with advanced vocabulary and complex structures (>=90 words)>\\n\"\n",
    "\"Text: E. <text with advanced vocabulary and complex structures (>=90 words)>\\n\"\n",
    "\"Text: F. <text with advanced vocabulary and complex structures (>=90 words)>\\n\"\n",
    "\"Text: G. <text with advanced vocabulary and complex structures (>=90 words)>\\n\"\n",
    "\"Text: H. <text with advanced vocabulary and complex structures (>=90 words)>\\n\"\n",
    "\n",
    "KG CONSTRAINTS:\n",
    "{kg_rules}\n",
    "\n",
    "EXAMPLE SUMMARIES (not full text):\n",
    "- Passage Example: \"Andrea Palladio\" — 920 words, 8 paragraphs, summary line included.\n",
    "- Passage Example: \"Father of Management\" — 880 words, 6 paragraphs, summary line included.\n",
    "- These examples are stored in the knowledge graph and can be fetched if needed.\n",
    "\"\"\"\n",
    "\n",
    "_QUESTION_TEMPLATE = \"\"\"SYSTEM: You are an IELTS question generator.\n",
    "\n",
    "PASSAGE:\n",
    "{passage}\n",
    "\n",
    "QUESTION TYPE RULES (from KG):\n",
    "{qtype_rules}\n",
    "\n",
    "TASK (strict):\n",
    "1) Produce exactly {count} questions of type {qtype_id}.\n",
    "2) Each question must be a JSON object with fields:\n",
    "   - id (string)\n",
    "   - question_text (string)\n",
    "   - options (list of 4 strings, labelled A/B/C/D)\n",
    "   - answer (one of \"A\",\"B\",\"C\",\"D\")\n",
    "   - rationale (short, 1-2 sentences why the answer is correct)\n",
    "   - linked_skills (list of skill ids)\n",
    "3) Distractors must follow KG distractor patterns.\n",
    "4) For questions with more than 2 answers, use at least 2 choices contain logical traps / distractors\n",
    "5) Output: a JSON array of question objects and nothing else.\n",
    "**You must output *only valid JSON*. No commentary. No trailing text.**\n",
    "\n",
    "EXAMPLE SUMMARIES (not full text):\n",
    "- Multiple Choice: 4 options (A–D), one correct answer, 1–2 sentence rationale.\n",
    "- True/False/Not Given: statements based on passage, mapped to T/F/NG.\n",
    "- Matching Headings: list of headings > number of paragraphs, not all used.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "_DISTRACTOR_TEMPLATE = \"\"\"SYSTEM: You are an IELTS distractor generator.\n",
    "\n",
    "PASSAGE:\n",
    "{passage}\n",
    "\n",
    "KG DISTRACTOR PATTERNS:\n",
    "{d_rules}\n",
    "\n",
    "EXAMPLE SUMMARIES OF DISTRACTOR TYPES:\n",
    "- Lexical Similarity: Wrong option looks similar in wording (synonyms, paraphrased terms).\n",
    "- Plausible but Wrong Detail: Option mentions something present in passage but not correct for the question.\n",
    "- Outside Knowledge Trap: Option is plausible using world knowledge but not supported in passage.\n",
    "- Opposite/Contradiction: Option states the reverse of what passage says.\n",
    "- Irrelevant but Related Topic: Option is thematically related but not directly answering.\n",
    "- Overgeneralisation: Option uses extreme or absolute wording not supported by passage.\n",
    "\n",
    "TASK:\n",
    "1) For each question (or for the passage in general), produce a short list of distractors following KG patterns.\n",
    "2) Output a JSON array of objects: \n",
    "   {{ \"for_question_id\": \"...\", \n",
    "   \"distractors\": [ {{ \"text\": \"...\", \"pattern\": \"...\" }}, ... ] }}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# Executors (public)\n",
    "# -----------------------\n",
    "def passage_executor(prompt_template: str, topic: str, outputs_so_far: Dict[str, Any]) -> str:\n",
    "    \"\"\"\n",
    "    Signature kept for compatibility with GEPA orchestration:\n",
    "      (prompt_template, topic, outputs_so_far)\n",
    "    Behavior: fetch sources (from outputs_so_far['sources'] if present, else call retriever inside),\n",
    "              fetch KG passage rules from MCP, generate and validate the passage (with retries).\n",
    "    Returns: passage text (string).\n",
    "    \"\"\"\n",
    "    # sources: prefer those passed in from main/orchestrator\n",
    "    sources = outputs_so_far.get(\"sources\")\n",
    "    if not sources:\n",
    "        # lazy import retriever here so executors can be imported standalone\n",
    "        try:\n",
    "            from retriever import retrieve_sources\n",
    "            sources = retrieve_sources(topic)\n",
    "        except Exception:\n",
    "            sources = []\n",
    "            logger.warning(\"retriever.retrieve_sources unavailable; proceeding without sources.\")\n",
    "\n",
    "    # flatten sources for prompt\n",
    "    sources_txt = \"\\n\".join([f\"[S{i+1}] {s.get('title','')}. {s.get('abstract', s.get('text',''))}\" \n",
    "                              for i, s in enumerate(sources[:6])])\n",
    "\n",
    "    kg_rules = MCP.get_passage_rules()\n",
    "    kg_rules_txt = json.dumps(kg_rules, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # attempts + validation loop\n",
    "    max_attempts = 3\n",
    "    threshold = 0.70  # passage validator score threshold\n",
    "    last_traces = None\n",
    "\n",
    "    for attempt in range(1, max_attempts + 1):\n",
    "        logger.info(\"PASSAGE_GENERATE attempt %d/%d for topic=%s\", attempt, max_attempts, topic)\n",
    "        prompt = _PASSAGE_TEMPLATE.format(topic=topic, sources=sources_txt, kg_rules=kg_rules_txt)\n",
    "\n",
    "        if DEBUG_STUB:\n",
    "            # useful for local debug runs\n",
    "            passage = f\"DEBUG PASSAGE about {topic}\\n\\nSummary: stub.\"\n",
    "        else:\n",
    "            passage = call_llm(prompt, tools=PASSAGE_TOOLS, expect_json=False, max_tokens=10000)\n",
    "            passage = maybe_call_examples(passage, \"fetch_passage_examples\")\n",
    "\n",
    "        # validate\n",
    "        score, raw_traces, fb_traces = validate_passage_text(passage)\n",
    "        logger.info(\"Passage score=%.3f raw=%s feedback=%s\", score, raw_traces, fb_traces)\n",
    "        last_traces = raw_traces\n",
    "\n",
    "        if score >= threshold:\n",
    "            return passage\n",
    "\n",
    "        fix_instructions = \"Please regenerate the passage and fix: \" + \"; \".join(fb_traces)\n",
    "        prompt += \"\\n\\nFEEDBACK: \" + fix_instructions\n",
    "\n",
    "\n",
    "        # small wait/backoff to avoid rate-limit spikes\n",
    "        time.sleep(0.5)\n",
    "\n",
    "    # After attempts exhausted: return last passage but mark in logs\n",
    "    logger.warning(\"PASSAGE generation failed to meet threshold after %d attempts. Returning last result.\", max_attempts)\n",
    "    return passage\n",
    "\n",
    "\n",
    "def questions_executor(prompt_template: str, topic: str, outputs_so_far: Dict[str, Any], qtype_id: str = \"Multiple_Choice\", count: int = 3) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Generate questions for a given passage present in outputs_so_far['passage'] (or raise).\n",
    "    Returns: list of question dicts (if JSON parsing fails, returns an empty list or raw text).\n",
    "    \"\"\"\n",
    "    passage = outputs_so_far.get(\"passage\")\n",
    "    if not passage:\n",
    "        raise ValueError(\"questions_executor requires 'passage' in outputs_so_far\")\n",
    "\n",
    "    qtype_rules = MCP.get_question_type_context(qtype_id)\n",
    "    qtype_rules_txt = json.dumps(qtype_rules, ensure_ascii=False, indent=2)\n",
    "\n",
    "    max_attempts = 2\n",
    "    threshold = 0.80  # question structure acceptance\n",
    "\n",
    "    for attempt in range(1, max_attempts + 1):\n",
    "        logger.info(\"QUESTIONS_GENERATE attempt %d/%d qtype=%s\", attempt, max_attempts, qtype_id)\n",
    "        prompt = _QUESTION_TEMPLATE.format(passage=passage, qtype_rules=qtype_rules_txt, qtype_id=qtype_id, count=count)\n",
    "\n",
    "        if DEBUG_STUB:\n",
    "            model_out = json.dumps([\n",
    "                {\"id\":\"Q1\",\"question_text\":\"DEBUG Q?\",\"options\":[\"A\",\"B\",\"C\",\"D\"],\"answer\":\"A\",\"rationale\":\"stub\",\"linked_skills\":[\"Skimming\"]}\n",
    "            ])\n",
    "        else:\n",
    "            model_out = call_llm(prompt, tools=QUESTION_TOOLS, expect_json=False, max_tokens=10000)\n",
    "            model_out = maybe_call_examples(model_out, \"fetch_questions_examples\", qtype_id=qtype_id)\n",
    "\n",
    "        try:\n",
    "            parsed = _parse_json_from_model(model_out)\n",
    "        except Exception as e:\n",
    "            logger.warning(\"Failed to parse questions JSON: %s\", e)\n",
    "            parsed = None\n",
    "\n",
    "        if isinstance(parsed, list):\n",
    "            q_score, q_raw, q_fb = validate_questions_structure(parsed)\n",
    "            logger.info(\"Questions structure score=%.3f raw=%s fb=%s\", q_score, q_raw, q_fb)\n",
    "            if q_score >= threshold:\n",
    "                return parsed\n",
    "            else:\n",
    "                prompt += \"\\n\\nFEEDBACK: \" + \"; \".join(q_fb)\n",
    "        else:\n",
    "            logger.warning(\"Questions generation not JSON list on attempt %d\", attempt)\n",
    "\n",
    "        time.sleep(0.4)\n",
    "\n",
    "    logger.warning(\"Failed to generate valid questions after %d attempts; returning best-effort parse\", max_attempts)\n",
    "    return parsed if parsed is not None else []\n",
    "\n",
    "\n",
    "def distractors_executor(prompt_template: str, topic: str, outputs_so_far: Dict[str, Any]) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Generate distractors aligned with KG patterns for the passage.\n",
    "    Returns: list of distractor objects.\n",
    "    \"\"\"\n",
    "    passage = outputs_so_far.get(\"passage\")\n",
    "    if not passage:\n",
    "        raise ValueError(\"distractors_executor requires 'passage' in outputs_so_far\")\n",
    "\n",
    "    d_rules = MCP.get_distractor_patterns()\n",
    "    d_rules_txt = json.dumps(d_rules, ensure_ascii=False, indent=2)\n",
    "\n",
    "    max_attempts = 2\n",
    "\n",
    "    for attempt in range(1, max_attempts + 1):\n",
    "        logger.info(\"DISTRACTORS_GENERATE attempt %d/%d\", attempt, max_attempts)\n",
    "        prompt = _DISTRACTOR_TEMPLATE.format(passage=passage, d_rules=d_rules_txt)\n",
    "        if DEBUG_STUB:\n",
    "            model_out = json.dumps([{\"for_question_id\":\"Q1\",\"distractors\":[{\"text\":\"DEBUG wrong\",\"pattern\":\"similar_lexical\"}]}])\n",
    "        else:\n",
    "            model_out = call_llm(prompt, expect_json=False, max_tokens=10000)\n",
    "\n",
    "        try:\n",
    "            parsed = _parse_json_from_model(model_out)\n",
    "            if isinstance(parsed, list):\n",
    "                return parsed\n",
    "        except Exception as e:\n",
    "            logger.warning(\"Failed to parse distractors JSON: %s\", e)\n",
    "\n",
    "        time.sleep(0.3)\n",
    "\n",
    "    logger.warning(\"Returning last distractors attempt result (may be invalid).\")\n",
    "    try:\n",
    "        return parsed if parsed is not None else []\n",
    "    except UnboundLocalError:\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39543dde",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# gepa.py - GEPA driver (multi-objective, Pareto-aware)\n",
    "import random\n",
    "import uuid\n",
    "from copy import deepcopy\n",
    "from typing import List, Dict, Callable, Any\n",
    "from statistics import mean\n",
    "import json, os\n",
    "from llm import call_llm\n",
    "from validators import score_passage_and_questions\n",
    "from logger import GEPA_Logger\n",
    "\n",
    "# ---------- CONFIG ----------\n",
    "MINIBATCH_SIZE = 4   #8 later on # small for sample efficiency\n",
    "NPARETO = 20 # 20 - 30% amount of topics\n",
    "MAX_CANDIDATES = 30 #30 - 50\n",
    "ROLLOUT_BUDGET = 200  #scale to 500 - 1000\n",
    "MUTATION_ATTEMPTS = 2 # 2 if more variety\n",
    "# ----------------------------\n",
    "\n",
    "MODULE_NAMES = [\"passage\", \"questions\"]\n",
    "\n",
    "\n",
    "def save_generation_passages(topic: str, outputs: dict):\n",
    "    # Sanitize topic name for filename\n",
    "    safe_topic = topic.replace(\" \", \"_\")\n",
    "\n",
    "    # Count how many generations already exist\n",
    "    existing = [\n",
    "        fn for fn in os.listdir(\".\")\n",
    "        if fn.startswith(f\"generated_{safe_topic}_\") and fn.endswith(\".json\")\n",
    "    ]\n",
    "    gen_num = len(existing) + 1\n",
    "\n",
    "    filename = f\"generated_{safe_topic}_{gen_num}.json\"\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(outputs, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"[SAVED] {filename}\")\n",
    "\n",
    "# ---------- Utilities ----------\n",
    "def new_candidate_from_base(base_prompts: Dict[str, str], extra_meta=None) -> Dict[str, Any]:\n",
    "    return {\n",
    "        \"id\": str(uuid.uuid4()),\n",
    "        \"prompts\": deepcopy(base_prompts),\n",
    "        \"scores\": {},      # last aggregated scores (dict)\n",
    "        \"meta\": extra_meta or {},\n",
    "        \"ancestry\": []\n",
    "    }\n",
    "\n",
    "\n",
    "# ---------- Rollout / evaluation ----------\n",
    "def run_rollout_on_topic(candidate: Dict, topic: str,\n",
    "                         executors: Dict[str, Callable[[str, str, dict], Any]],\n",
    "                         verbose: bool = False) -> Dict:\n",
    "    \"\"\"\n",
    "    Run the full pipeline for one candidate on a single topic.\n",
    "    executors: mapping module_name -> function(prompt_template, topic, outputs_so_far) -> output\n",
    "    Returns { topic, outputs, scores (dict), traces }\n",
    "    \"\"\"\n",
    "    outputs: Dict[str, Any] = {}\n",
    "    for m in MODULE_NAMES:\n",
    "        prompt_text = candidate[\"prompts\"].get(m)\n",
    "        if prompt_text is None:\n",
    "            raise RuntimeError(f\"Candidate missing prompt for module {m}\")\n",
    "\n",
    "        out = executors[m](prompt_text, topic, outputs)\n",
    "        outputs[m] = out\n",
    "\n",
    "    # validators.score_passage_and_questions now returns (scores_dict, traces)\n",
    "    scores, traces_dict = score_passage_and_questions(outputs, topic)\n",
    "    candidate[\"scores\"] = scores\n",
    "    candidate[\"traces\"] = traces_dict[\"raw\"]   # keep raw for debug\n",
    "    candidate[\"fb_traces\"] = traces_dict[\"feedback\"]  # feedback for GEPA\n",
    "\n",
    "    save_generation_passages(topic, outputs)\n",
    "    return {\"topic\": topic, \"outputs\": outputs, \"scores\": scores, \"raw_traces\": traces_dict[\"raw\"], \"fb_traces\": traces_dict[\"feedback\"]}\n",
    "\n",
    "\n",
    "def run_minibatch(candidate: Dict, topics: List[str],\n",
    "                  executors: Dict[str, Callable[[str, str, dict], Any]]) -> List[Dict]:\n",
    "    results = []\n",
    "    for t in topics:\n",
    "        res = run_rollout_on_topic(candidate, t, executors)\n",
    "        results.append(res)\n",
    "    return results\n",
    "\n",
    "\n",
    "# ---------- Multi-objective helpers ----------\n",
    "def dominates(a: Dict[str, float], b: Dict[str, float]) -> bool:\n",
    "    \"\"\"\n",
    "    A dominates B if:\n",
    "      - for every objective k, a[k] >= b[k]\n",
    "      - and for at least one k, a[k] > b[k]\n",
    "    Missing keys are treated as 0.0.\n",
    "    \"\"\"\n",
    "    keys = set(a.keys()) | set(b.keys())\n",
    "    ge_all = all(a.get(k, 0.0) >= b.get(k, 0.0) for k in keys)\n",
    "    gt_some = any(a.get(k, 0.0) > b.get(k, 0.0) for k in keys)\n",
    "    return ge_all and gt_some\n",
    "\n",
    "\n",
    "def aggregate_scores(results: List[Dict]) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Given a list of rollout results (each result has 'scores' dict),\n",
    "    compute the average per objective across the list.\n",
    "    \"\"\"\n",
    "    if not results:\n",
    "        return {}\n",
    "    # collect union of keys\n",
    "    keys = set()\n",
    "    for r in results:\n",
    "        keys.update(r[\"scores\"].keys())\n",
    "    agg = {}\n",
    "    for k in keys:\n",
    "        vals = [r[\"scores\"].get(k, 0.0) for r in results]\n",
    "        agg[k] = mean(vals) if vals else 0.0\n",
    "    return agg\n",
    "\n",
    "\n",
    "def build_pareto_front(records: Dict[str, Dict[str, Dict[str, float]]]) -> List[str]:\n",
    "    \"\"\"\n",
    "    records: { candidate_id: { topic: scores_dict, ... }, ... }\n",
    "    Compute per-candidate average vector across topics, then return list of\n",
    "    non-dominated candidate ids (Pareto front).\n",
    "    \"\"\"\n",
    "    # compute average vector per candidate\n",
    "    avg_vectors: Dict[str, Dict[str, float]] = {}\n",
    "    for cid, topic_map in records.items():\n",
    "        # topic_map: {topic: scores_dict}\n",
    "        topic_results = list(topic_map.values())\n",
    "        if not topic_results:\n",
    "            avg_vectors[cid] = {}\n",
    "            continue\n",
    "        # aggregate across topics\n",
    "        normed = []\n",
    "        for s in topic_results:\n",
    "            if isinstance(s, dict):\n",
    "                normed.append({\"scores\": s})\n",
    "            else:\n",
    "                normed.append({\"scores\": {\"_single\": float(s)}})\n",
    "        avg_vectors[cid] = aggregate_scores(normed)\n",
    "\n",
    "    # compute non-dominated set\n",
    "    pareto = []\n",
    "    for a in avg_vectors:\n",
    "        a_vec = avg_vectors[a]\n",
    "        dominated = False\n",
    "        for b in avg_vectors:\n",
    "            if a == b:\n",
    "                continue\n",
    "            b_vec = avg_vectors[b]\n",
    "            if dominates(b_vec, a_vec):\n",
    "                dominated = True\n",
    "                break\n",
    "        if not dominated:\n",
    "            pareto.append(a)\n",
    "    return pareto\n",
    "\n",
    "\n",
    "# ---------- GEPA meta-prompt ----------\n",
    "GEPA_META_PROMPT_TEMPLATE = \"\"\"\n",
    "I provided an assistant with the following instruction (the module prompt) delimited by triple quotes:\n",
    "\n",
    "'''\n",
    "{current_instruction}\n",
    "'''\n",
    "\n",
    "Here are a few examples of inputs, outputs, and feedback from runs:\n",
    "\n",
    "{examples_text}\n",
    "\n",
    "Your task: write a new improved instruction for the assistant (the same module) that\n",
    "- fixes the problems called out in the feedback,\n",
    "- includes any domain-specific constraints implied by the examples,\n",
    "- is explicit and repeatable,\n",
    "- keep it concise.\n",
    "\n",
    "Return ONLY the new instruction inside triple backticks.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def make_meta_prompt(current_instruction: str, examples: List[Dict]) -> str:\n",
    "    ex_texts = []\n",
    "    for ex in examples:\n",
    "        ex_texts.append(f\"Input: {ex['input']}\\nOutput: {ex['output']}\\nFeedback: {ex['feedback']}\\n---\")\n",
    "    return GEPA_META_PROMPT_TEMPLATE.format(current_instruction=current_instruction,\n",
    "                                            examples_text=\"\\n\".join(ex_texts))\n",
    "\n",
    "\n",
    "def reflective_prompt_mutation(module_name: str, current_prompt: str, examples: List[Dict]) -> str:\n",
    "    meta = make_meta_prompt(current_prompt, examples)\n",
    "    resp = call_llm(meta)\n",
    "    new_instr = resp.strip()\n",
    "    if new_instr.startswith(\"```\") and new_instr.endswith(\"```\"):\n",
    "        new_instr = new_instr.strip(\"`\").strip()\n",
    "    return new_instr\n",
    "\n",
    "\n",
    "def system_merge_prompts(prompt_a: str, prompt_b: str) -> str:\n",
    "    lines = []\n",
    "    for s in (prompt_a + \"\\n\" + prompt_b).splitlines():\n",
    "        s_clean = s.strip()\n",
    "        if not s_clean:\n",
    "            continue\n",
    "        if s_clean not in lines:\n",
    "            lines.append(s_clean)\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "# ---------- Candidate pool ----------\n",
    "class CandidatePool:\n",
    "    def __init__(self, base_prompts: Dict[str, str], max_size=MAX_CANDIDATES):\n",
    "        self.base_prompts = deepcopy(base_prompts)\n",
    "        self.pool: Dict[str, Dict] = {}\n",
    "        self.max_size = max_size   # <-- set this first\n",
    "        c = new_candidate_from_base(base_prompts, extra_meta={\"seed\": \"base\"})\n",
    "        self.add_candidate(c)\n",
    "\n",
    "    def add_candidate(self, cand: Dict):\n",
    "        self.pool[cand[\"id\"]] = cand\n",
    "        self._trim_pool()\n",
    "\n",
    "    def remove_candidate(self, cid: str):\n",
    "        if cid in self.pool:\n",
    "            del self.pool[cid]\n",
    "\n",
    "    def _trim_pool(self):\n",
    "        if len(self.pool) > self.max_size:\n",
    "            while len(self.pool) > self.max_size:\n",
    "                key = random.choice(list(self.pool.keys()))\n",
    "                del self.pool[key]\n",
    "\n",
    "    def list_candidates(self) -> List[Dict]:\n",
    "        return list(self.pool.values())\n",
    "\n",
    "\n",
    "# ---------- GEPA main loop ----------\n",
    "def gepa_optimize(\n",
    "    executors: Dict[str, Callable[[str, str, dict], Any]],\n",
    "    base_prompts: Dict[str, str],\n",
    "    topics: List[str],\n",
    "    dpareto_size: int = NPARETO,\n",
    "    budget: int = ROLLOUT_BUDGET\n",
    "):\n",
    "    pool = CandidatePool(base_prompts)\n",
    "    random.shuffle(topics)\n",
    "    dfeedback = topics[:min(len(topics), 200)]\n",
    "    dpareto = topics[:min(len(topics), dpareto_size)]\n",
    "    logger = GEPA_Logger()\n",
    "    rollouts_used = 0\n",
    "    iteration = 0\n",
    "    # records: candidate_id -> topic -> scores_dict\n",
    "    records: Dict[str, Dict[str, Dict[str, float]]] = {}\n",
    "    for c in pool.list_candidates():\n",
    "        records[c[\"id\"]] = {}\n",
    "\n",
    "    while rollouts_used < budget:\n",
    "        iteration += 1\n",
    "        print(f\"[GEPA] iter {iteration}, rollouts_used {rollouts_used}, pool_size {len(pool.pool)}\")\n",
    "\n",
    "        candidates = pool.list_candidates()\n",
    "        parent = random.choice(candidates)\n",
    "        module_to_mutate = random.choice(MODULE_NAMES)\n",
    "        minibatch = random.sample(dfeedback, k=min(MINIBATCH_SIZE, len(dfeedback)))\n",
    "\n",
    "        # evaluate parent on minibatch\n",
    "        parent_results = run_minibatch(parent, minibatch, executors)\n",
    "        rollouts_used += len(parent_results)\n",
    "        rec = records.setdefault(parent[\"id\"], {})\n",
    "        for r in parent_results:\n",
    "            rec[r[\"topic\"]] = r[\"scores\"]\n",
    "\n",
    "        # prepare examples for meta-prompt\n",
    "        examples = []\n",
    "        for r in parent_results:\n",
    "            passage = r[\"outputs\"].get(\"passage\", \"\")\n",
    "            out_summary = (passage[:200].replace(\"\\n\", \" \")) if isinstance(passage, str) else str(passage)\n",
    "            fb_text = \"; \".join(r.get(\"fb_traces\", []))  # only feedback traces for mutation\n",
    "            examples.append({\"input\": r[\"topic\"], \"output\": out_summary, \"feedback\": fb_text})\n",
    "\n",
    "\n",
    "        # attempt mutation\n",
    "        new_prompts: Dict[str, str] = {}\n",
    "        for attempt in range(MUTATION_ATTEMPTS):\n",
    "            current_instr = parent[\"prompts\"][module_to_mutate]\n",
    "            mutated = reflective_prompt_mutation(module_to_mutate, current_instr, examples)\n",
    "            if mutated and len(mutated) > 20:\n",
    "                new_prompts[module_to_mutate] = mutated\n",
    "                break\n",
    "        if not new_prompts:\n",
    "            print(\"[GEPA] no mutation produced; skipping.\")\n",
    "            continue\n",
    "\n",
    "        # build child candidate\n",
    "        child = deepcopy(parent)\n",
    "        child[\"id\"] = str(uuid.uuid4())\n",
    "        child[\"prompts\"] = deepcopy(parent[\"prompts\"])\n",
    "        child[\"prompts\"].update(new_prompts)\n",
    "        child[\"ancestry\"] = parent.get(\"ancestry\", []) + [parent[\"id\"]]\n",
    "\n",
    "        # evaluate child on minibatch\n",
    "        child_results = run_minibatch(child, minibatch, executors)\n",
    "        rollouts_used += len(child_results)\n",
    "\n",
    "        # aggregate per-objective means across minibatch\n",
    "        parent_vec = aggregate_scores(parent_results)\n",
    "        child_vec = aggregate_scores(child_results)\n",
    "        print(f\"[GEPA] parent_vec {parent_vec}\")\n",
    "        print(f\"[GEPA] child_vec  {child_vec}\")\n",
    "\n",
    "        # require Pareto dominance on minibatch to consider extended evaluation\n",
    "        if not dominates(child_vec, parent_vec):\n",
    "            print(\"[GEPA] Child did not dominate parent on minibatch; rejecting.\")\n",
    "            continue\n",
    "\n",
    "        # extended Pareto evaluation\n",
    "        pareto_eval_topics = random.sample(dpareto, k=min(len(dpareto), max(4, dpareto_size // 5)))\n",
    "        parent_pareto = run_minibatch(parent, pareto_eval_topics, executors)\n",
    "        child_pareto = run_minibatch(child, pareto_eval_topics, executors)\n",
    "        rollouts_used += len(pareto_eval_topics) * 2\n",
    "\n",
    "        parent_p_vec = aggregate_scores(parent_pareto)\n",
    "        child_p_vec = aggregate_scores(child_pareto)\n",
    "        print(f\"[GEPA] parent_p_vec {parent_p_vec}\")\n",
    "        print(f\"[GEPA] child_p_vec  {child_p_vec}\")\n",
    "\n",
    "        if dominates(child_p_vec, parent_p_vec):\n",
    "            print(f\"[GEPA] Accepting child {child['id']}.\")\n",
    "            pool.add_candidate(child)\n",
    "            # update records for pareto set\n",
    "            records[child[\"id\"]] = {r[\"topic\"]: r[\"scores\"] for r in child_pareto}\n",
    "            records[parent[\"id\"]] = {r[\"topic\"]: r[\"scores\"] for r in parent_pareto}\n",
    "        else:\n",
    "            print(\"[GEPA] Child failed on pareto set; rejecting.\")\n",
    "\n",
    "        if iteration % 10 == 0:\n",
    "            pareto_ids = build_pareto_front(records)\n",
    "            print(f\"[GEPA] pareto front size: {len(pareto_ids)}\")\n",
    "        logger.log({\n",
    "            \"iteration\": iteration,\n",
    "            \"parent_id\": parent[\"id\"],\n",
    "            \"child_id\": child[\"id\"],\n",
    "            \"parent_scores\": parent.get(\"scores\", {}),\n",
    "            \"child_scores\": child.get(\"scores\", {}),\n",
    "            \"examples\": examples,\n",
    "            \"traces\": child.get(\"traces\", [])\n",
    "        })\n",
    "\n",
    "    print(\"[GEPA] Budget exhausted.\")\n",
    "    return pool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64656a6d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# main.py (GEPA-only orchestrator, using unified validators.py)\n",
    "\n",
    "import pprint\n",
    "from retriever import retrieve_sources\n",
    "from executors import passage_executor, questions_executor, distractors_executor\n",
    "from gepa import gepa_optimize, build_pareto_front\n",
    "from validators import score_passage_and_questions  # unified scoring + validation\n",
    "\n",
    "\n",
    "executors = {\n",
    "    \"passage\": passage_executor,\n",
    "    \"questions\": questions_executor\n",
    "    #\"distractors\": distractors_executor,\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# --- GEPA Run ---\n",
    "def run_with_gepa():\n",
    "    base_prompts = {\n",
    "        \"passage\": (\n",
    "            \"SYSTEM: You are an IELTS Academic Reading passage generator.\\n\\n\"\n",
    "            \"TASK:\\n\"\n",
    "            \"- Write ONE IELTS Academic Reading passage about the given topic.\\n\"\n",
    "            \"- Length: 800–1000 words (ideal ~900).\\n\"\n",
    "            \"- Structure: 4–6 paragraphs, each separated by a blank line.\\n\"\n",
    "            \"- Style: academic, formal, factual, not conversational.\\n\"\n",
    "            \"- At the very end, include a single line starting with exactly:\\n\"\n",
    "            \"  Summary: <short one-sentence summary of the passage>\\n\\n\"\n",
    "            \"If you cannot follow ALL requirements, regenerate until you do.\"\n",
    "        ),\n",
    "        \"questions\": (\n",
    "            \"SYSTEM: You are an IELTS Multiple Choice Question (MCQ) generator.\\n\\n\"\n",
    "            \"TASK:\\n\"\n",
    "            \"- Write 3 IELTS-style MCQs based on the passage.\\n\"\n",
    "            \"- Each question must have exactly 4 options (A, B, C, D).\\n\"\n",
    "            \"- Provide exactly one correct answer.\\n\"\n",
    "            \"- Output MUST be valid JSON array (and nothing else).\\n\"\n",
    "            \"- Use this schema:\\n\\n\"\n",
    "            \"[\\n\"\n",
    "            \"  {\\n\"\n",
    "            \"    \\\"id\\\": \\\"Q1\\\",\\n\"\n",
    "            \"    \\\"stem\\\": \\\"What is the main purpose of ...?\\\",\\n\"\n",
    "            \"    \\\"options\\\": [\\\"Option A\\\", \\\"Option B\\\", \\\"Option C\\\", \\\"Option D\\\"],\\n\"\n",
    "            \"    \\\"answer\\\": \\\"B\\\"\\n\"\n",
    "            \"  },\\n\"\n",
    "            \"  {\\n\"\n",
    "            \"    \\\"id\\\": \\\"Q2\\\",\\n\"\n",
    "            \"    \\\"stem\\\": \\\"According to the passage, ...\\\",\\n\"\n",
    "            \"    \\\"options\\\": [\\\"Option A\\\", \\\"Option B\\\", \\\"Option C\\\", \\\"Option D\\\"],\\n\"\n",
    "            \"    \\\"answer\\\": \\\"C\\\"\\n\"\n",
    "            \"  },\\n\"\n",
    "            \"  {\\n\"\n",
    "            \"    \\\"id\\\": \\\"Q3\\\",\\n\"\n",
    "            \"    \\\"stem\\\": \\\"Which of the following is true ...?\\\",\\n\"\n",
    "            \"    \\\"options\\\": [\\\"Option A\\\", \\\"Option B\\\", \\\"Option C\\\", \\\"Option D\\\"],\\n\"\n",
    "            \"    \\\"answer\\\": \\\"D\\\"\\n\"\n",
    "            \"  }\\n\"\n",
    "            \"]\\n\\n\"\n",
    "            \"STRICT: Output must be valid JSON only, no explanations, no extra text.\"\n",
    "        )\n",
    "    }\n",
    "\n",
    "\n",
    "    topics = [\n",
    "        \"Artificial Intelligence in Education\",\n",
    "        \"Climate Change and Policy\",\n",
    "        \"Globalization and Culture\",\n",
    "        \"Renewable Energy Technologies\",\n",
    "    ]\n",
    "\n",
    "    print(\"[GEPA] starting optimization (budget=20 demo)\")\n",
    "    pool = gepa_optimize(executors, base_prompts, topics, budget=20)\n",
    "\n",
    "    # --- Pareto front from final pool ---\n",
    "    records_for_pareto = {cid: cand.get(\"scores\", {}) for cid, cand in pool.pool.items()}\n",
    "    pareto_ids = build_pareto_front(records_for_pareto)\n",
    "\n",
    "    print(\"\\n=== GEPA Final Pareto Front ===\")\n",
    "    for cid in pareto_ids:\n",
    "        cand = pool.pool[cid]\n",
    "        print(f\"Candidate {cid[:8]} (ancestry: {cand.get('ancestry', [])})\")\n",
    "        print(\"Scores:\")\n",
    "        pprint.pprint(cand.get(\"scores\", {}))\n",
    "        print(\"Prompts (snippets):\")\n",
    "        for m, txt in cand.get(\"prompts\", {}).items():\n",
    "            snippet = (txt[:300] + \"...\") if isinstance(txt, str) and len(txt) > 300 else txt\n",
    "            print(f\" - {m}: {snippet}\")\n",
    "        print(\"------\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_with_gepa()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
